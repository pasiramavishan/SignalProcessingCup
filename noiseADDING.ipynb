{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from IPython.display import Audio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_audio_sample_path(index):\n",
    "        fold = f\"spk_{annotations.iloc[index, 5]}\"\n",
    "        audio_dir = r\"C:\\Users\\pasir\\Desktop\\data - Copy\\single-channel\\enrollment\"\n",
    "        path = os.path.join(audio_dir, fold, annotations.iloc[index, 0])\n",
    "        return path\n",
    "def _cut_if_necessary(signal1, signal2):\n",
    "        if signal2.shape[1] > signal1.shape[1]:\n",
    "            signal2 = signal2[:, :signal1.shape[1]]\n",
    "        return signal2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = r\"C:\\Users\\pasir\\Desktop\\data - Copy\\Book1.csv\"\n",
    "annotations = pd.read_csv(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40506])\n"
     ]
    }
   ],
   "source": [
    "# def _right_pad_if_necessary(signal1, signal2):\n",
    "#         length_signal = signal1.shape[1]\n",
    "#         if length_signal < signal2.shape[1]:\n",
    "#             num_missing_samples = signal2.shape[1] - length_signal\n",
    "#             last_dim_padding = (0, num_missing_samples)\n",
    "#             signal1 = torch.nn.functional.pad(signal1, last_dim_padding)\n",
    "#         return signal1\n",
    "# noise = noise[:, : speech.shape[1]]\n",
    "\n",
    "\n",
    "noise, _ = torchaudio.load(r\"C:\\Users\\pasir\\Desktop\\Noise adding\\0000f88619.wav\")\n",
    "\n",
    "print(noise.shape)\n",
    "\n",
    "for index in range(225):\n",
    "    path = _get_audio_sample_path(index)\n",
    "    speech, _ = torchaudio.load(path)\n",
    "    noise =  noise.repeat(1, 20)\n",
    "\n",
    "    noise = _cut_if_necessary(speech, noise)\n",
    "\n",
    "\n",
    "    \n",
    "    snr_dbs = torch.tensor([20, 10, 3])\n",
    "    noisy_speeches = F.add_noise(speech, noise, snr_dbs)\n",
    "    snr_db, noisy_speech = snr_dbs[2], noisy_speeches[2:3]\n",
    "    \n",
    "    path = path + f\"SNR{index%3}.wav\"\n",
    "    torchaudio.save(path, noisy_speeches, 16000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
